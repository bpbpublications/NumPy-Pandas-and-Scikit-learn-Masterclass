{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85a24d22",
   "metadata": {},
   "source": [
    "# Responsible Pipeline Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee9e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c840a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import time\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd4774",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26621ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/adult.csv', \n",
    "                 na_values=' ?', \n",
    "                 skipinitialspace=True)\n",
    "df['income'] = df['income'].str.strip()\n",
    "df = df.dropna(subset=['income', 'sex', 'race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907cb944",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['income'].map({'>50K': 1, '<=50K': 0})\n",
    "df = df[~y.isna()]  \n",
    "X = df.drop(['income'], axis=1, errors='ignore')\n",
    "sensitive = df[['sex', 'race']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0e9aee",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0634a347",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test, \n",
    " sens_train, sens_test) = train_test_split(\n",
    "    X, y, sensitive, test_size=0.3, \n",
    "    stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f753d478",
   "metadata": {},
   "source": [
    "## Define preprocessing and model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b833bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numerical_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('clf', LogisticRegression(solver='liblinear'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848bb887",
   "metadata": {},
   "source": [
    "## CV and fairness audits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ad36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "metrics = {'accuracy': [], 'recall': [], 'precision': [], 'f1': []}\n",
    "subgroup_performance = []\n",
    "\n",
    "for train_idx, valid_idx in skf.split(X_train, y_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "    sens_val = sens_train.iloc[valid_idx]\n",
    "    pipeline.fit(X_tr, y_tr)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    metrics['accuracy'].append(accuracy_score(y_val, y_pred))\n",
    "    metrics['recall'].append(recall_score(y_val, y_pred))\n",
    "    metrics['precision'].append(precision_score(y_val, y_pred))\n",
    "    metrics['f1'].append(f1_score(y_val, y_pred))\n",
    "\n",
    "    # -- Attribute-level audits --\n",
    "    for attr in sens_val.columns:\n",
    "        for grp in sens_val[attr].unique():\n",
    "            mask = sens_val[attr] == grp\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "            subgroup_performance.append({\n",
    "                'type': 'attribute',\n",
    "                'attribute': attr,\n",
    "                'group': grp,\n",
    "                'accuracy': accuracy_score(y_val[mask], y_pred[mask])\n",
    "            })\n",
    "    # -- Intersectional audits --\n",
    "    intersection_series = sens_val['sex'].astype(str) + \"_\" + sens_val['race'].astype(str)\n",
    "    for grp in intersection_series.unique():\n",
    "        mask = intersection_series == grp\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        subgroup_performance.append({\n",
    "            'type': 'intersectional',\n",
    "            'attribute': 'sex_race',\n",
    "            'group': grp,\n",
    "            'accuracy': accuracy_score(y_val[mask], y_pred[mask])\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ec258",
   "metadata": {},
   "source": [
    "## Fit the model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e262d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "overall = {\n",
    "    'accuracy': accuracy_score(y_test, y_test_pred),\n",
    "    'recall': recall_score(y_test, y_test_pred),\n",
    "    'precision': precision_score(y_test, y_test_pred),\n",
    "    'f1': f1_score(y_test, y_test_pred)\n",
    "}\n",
    "print(\"Overall test metrics:\", overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29705a2",
   "metadata": {},
   "source": [
    "## Test subgroup accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "sens_test_combined = sens_test['sex'].astype(str) + \"_\" + sens_test['race'].astype(str)\n",
    "\n",
    "groups = []\n",
    "accuracies = []\n",
    "f1s = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for grp in sens_test_combined.unique():\n",
    "    mask = sens_test_combined == grp\n",
    "    acc = accuracy_score(y_test[mask], y_test_pred[mask])\n",
    "    f1 = f1_score(y_test[mask], y_test_pred[mask])\n",
    "    precision = precision_score(y_test[mask], y_test_pred[mask])\n",
    "    recall = recall_score(y_test[mask], y_test_pred[mask])\n",
    "    groups.append(grp)\n",
    "    accuracies.append(acc)\n",
    "    f1s.append(f1)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c02dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame({\n",
    "    'Group': groups,\n",
    "    'Accuracy': accuracies,\n",
    "    'F1': f1s,\n",
    "    'Precision': precisions,\n",
    "    'Recall': recalls\n",
    "})\n",
    "df_metrics = df_metrics.set_index('Group')\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_group_metrics_bar(\n",
    "    df,\n",
    "    metrics,\n",
    "    palette_name=\"Greys\",\n",
    "    sns_theme=None,\n",
    "    legend_pos=None,\n",
    "    legend_args=None,\n",
    "    label_args=None,\n",
    "    x_tick_rotation=90,\n",
    "    figsize=(12, 6),\n",
    "    show_legend=True,\n",
    "    **kwargs\n",
    "):\n",
    "    if sns_theme is not None:\n",
    "        sns.set_theme(style=sns_theme)\n",
    "    else:\n",
    "        sns.set_theme()\n",
    "    palette = sns.color_palette(palette_name, n_colors=len(metrics))\n",
    "    x = np.arange(len(df))\n",
    "    width = 0.8 / len(metrics)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax.bar(x + i * width, df[metric], width, label=metric, color=palette[i])\n",
    "\n",
    "    ax.set_xticks(x + width * (len(metrics) - 1) / 2)\n",
    "    ax.set_xticklabels(df.index, rotation=x_tick_rotation, fontsize=kwargs.get(\"xtick_fontsize\", 12))\n",
    "\n",
    "    if label_args is not None:\n",
    "        ax.set_ylabel(**label_args)\n",
    "    else:\n",
    "        ax.set_ylabel(kwargs.get(\"ylabel\", \"Score\"))\n",
    "\n",
    "    ax.set_title(kwargs.get(\"title\", \"Model Performance Metrics by Group\"))\n",
    "\n",
    "    if show_legend:\n",
    "        if legend_args is not None:\n",
    "            if legend_pos is not None:\n",
    "                ax.legend(loc=legend_pos, **legend_args)\n",
    "            else:\n",
    "                ax.legend(**legend_args)\n",
    "        else:\n",
    "            ax.legend()\n",
    "\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de1a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_group_metrics_bar(\n",
    "    df_metrics, ['Accuracy', 'F1', \n",
    "                'Precision', \n",
    "                'Recall'],\n",
    "    sns_theme=\"whitegrid\",\n",
    "    show_legend=True,\n",
    "    \n",
    "    label_args={'ylabel': 'Score', 'fontsize': 12}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6636a4a8",
   "metadata": {},
   "source": [
    "## SHAP Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b564f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "X_sample = X_train.sample(n=100, random_state=1)\n",
    "X_sample_proc = pipeline.named_steps['preprocessing'].transform(X_sample)\n",
    "explainer = shap.Explainer(pipeline.named_steps['clf'], X_sample_proc)\n",
    "shap_values = explainer(pipeline.named_steps['preprocessing'].transform(X_test[:100]))\n",
    "shap.summary_plot(shap_values, feature_names=pipeline.named_steps['preprocessing'].get_feature_names_out(), show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ca0f71",
   "metadata": {},
   "source": [
    "## Flag uncertain predictions for human review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dbe696",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = pipeline.predict_proba(X_test)[:, 1]\n",
    "edge_cases = np.where((probs > 0.45) & (probs < 0.55))[0]\n",
    "X_test.iloc[edge_cases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997a3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Edge cases needing manual review:\", len(edge_cases))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f6f56b",
   "metadata": {},
   "source": [
    "## Documentation and audit trail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7261a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'features': list(X.columns),\n",
    "    'sensitive_features': list(sensitive.columns),\n",
    "    'model_params': pipeline.named_steps['clf'].get_params(),\n",
    "    'cv_metrics': metrics,\n",
    "    'overall_metrics': overall,\n",
    "    'subgroup_performance': subgroup_performance\n",
    "}\n",
    "print(metadata)\n",
    "os.makedirs('audit', exist_ok=True)\n",
    "joblib.dump({'pipeline': pipeline, \n",
    "             'metadata': metadata}, \n",
    "             'audit/responsible_model_audit.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-book-py-12-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
