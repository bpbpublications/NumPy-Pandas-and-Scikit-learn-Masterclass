{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_df = pd.read_csv('data/synth_customer_churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_dataframe(df: pd.DataFrame):\n",
    "    return df.style.set_table_styles([\n",
    "        # Header styling\n",
    "        {\"selector\": \"thead th\", \"props\": [\n",
    "            (\"background-color\", \"#f2f2f2\"),  \n",
    "            (\"color\", \"black\"),              \n",
    "            (\"font-weight\", \"bold\"),         \n",
    "            (\"border\", \"1px solid #ddd\"),    \n",
    "            (\"text-align\", \"center\")        \n",
    "        ]},\n",
    "        # Body styling\n",
    "        {\"selector\": \"tbody td\", \"props\": [\n",
    "            (\"background-color\", \"white\"),   \n",
    "            (\"color\", \"black\"),              \n",
    "            (\"border\", \"1px solid #ddd\"),    \n",
    "            (\"text-align\", \"center\")         \n",
    "        ]}\n",
    "    ]).set_properties(**{\n",
    "        \"border-collapse\": \"collapse\",      \n",
    "        \"font-size\": \"12px\",                \n",
    "        \"font-family\": \"Arial, sans-serif\" \n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA (Exploratory data analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install modelviz seaborn pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelviz.histogram import plot_feature_histograms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_histograms(cc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom module in supporting GitHub repository, using functions built\n",
    "# In previous chapters\n",
    "from multiclass.class_utils.missing_values import missing_values_summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props_df, _ = missing_values_summarizer(df=cc_df)\n",
    "style_dataframe(props_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_class_imbalance(df, column, \n",
    "                              palette=\"Set2\",\n",
    "                              plt_title='Class imbalance',\n",
    "                              xlabel='Class', \n",
    "                              ylabel='Frequency',\n",
    "                              label_fontsize=12,\n",
    "                              title_fontsize=14,\n",
    "                              tick_fontsize=12,\n",
    "                              text_fontsize=12,\n",
    "                              text_halignment='center',\n",
    "                              figsize=(8,6)):\n",
    "    \n",
    "    class_counts = df[column].value_counts()\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    bar_plot = sns.barplot(\n",
    "        x=class_counts.index,\n",
    "        y=class_counts.values,\n",
    "        hue=class_counts.values,\n",
    "        palette=palette\n",
    "    )\n",
    "\n",
    "    # Add the value labels to the bars\n",
    "    for i, value in enumerate(class_counts.values):\n",
    "        bar_plot.text(i, value + 0.5, str(value), \n",
    "                      ha=text_halignment, \n",
    "                      fontsize=text_fontsize)\n",
    "    \n",
    "    plt.title(plt_title, fontsize=title_fontsize)\n",
    "    plt.xlabel(xlabel, fontsize=label_fontsize)\n",
    "    plt.ylabel(ylabel, fontsize=label_fontsize)\n",
    "    plt.xticks(fontsize=tick_fontsize)\n",
    "    plt.yticks(fontsize=tick_fontsize)\n",
    "    sns.despine()\n",
    "    bar_plot.get_legend().remove() if bar_plot.get_legend() else None\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_class_imbalance(df=cc_df, \n",
    "                          column='ChurnCategory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = cc_df.drop(columns=['ChurnCategory', 'CustomerID'])  \n",
    "X['CustomerSupportCalls'].astype(float)\n",
    "X_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cc_df['ChurnCategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    'Low Risk': 0,\n",
    "    'Medium Risk': 1,\n",
    "    'High Risk': 2\n",
    "}\n",
    "y_mapped = y.map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y_mapped, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train, columns=X_cols)\n",
    "X_test_df = pd.DataFrame(X_test, columns=X_cols)\n",
    "len(X_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modal Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "def modal_impute(train_df, test_df, columns):\n",
    "    imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "    train_df[columns] = imputer.fit_transform(train_df[columns])\n",
    "    test_df[columns] = imputer.transform(test_df[columns])\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_impute = list(X_train_df.select_dtypes(include=[\"object\"]).columns)\n",
    "cols_to_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modal_X_train, modal_X_test = modal_impute(X_train_df, \n",
    "                                           X_test_df, \n",
    "                                           cols_to_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props_df, _ = missing_values_summarizer(df=modal_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_dataframe(props_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute and scale with `ColumnTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (MinMaxScaler, \n",
    "                                   StandardScaler, \n",
    "                                   RobustScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('age_pipeline', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', MinMaxScaler())\n",
    "        ]), ['Age']),\n",
    "        ('tenure_pipeline', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', RobustScaler())\n",
    "        ]), ['Tenure']),\n",
    "        ('monthlycharges_pipeline', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), ['MonthlyCharges']),\n",
    "        ('serviceusage_pipeline', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', MinMaxScaler())\n",
    "        ]), ['ServiceUsage']),\n",
    "        ('customer_service_pipeline', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), ['CustomerSupportCalls'])\n",
    "    ],\n",
    "    remainder='passthrough'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = preprocessor.fit_transform(modal_X_train)\n",
    "X_test_transformed = preprocessor.transform(modal_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cols = ['Age', 'Tenure', 'MonthlyCharges', \n",
    "                 'ServiceUsage', 'CustomerSupportCalls',\n",
    "                 'Gender', 'ContractType', 'PaymentMethod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed_df = pd.DataFrame(X_train_transformed, \n",
    "                                      columns=pipeline_cols, \n",
    "                                      index=modal_X_train.index)\n",
    "\n",
    "X_test_transformed_df = pd.DataFrame(X_test_transformed, \n",
    "                                     columns=pipeline_cols, \n",
    "                                     index=modal_X_test.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_dataframe(X_train_transformed_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Gender', 'ContractType', 'PaymentMethod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dummies = pd.get_dummies(X_train_transformed_df[cat_cols], drop_first=True).astype(int)\n",
    "test_dummies = pd.get_dummies(X_test_transformed_df[cat_cols], drop_first=True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(train_dummies.columns) != len(test_dummies.columns):\n",
    "    assert ValueError('Expected the columns to match when encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat(\n",
    "    [X_train_transformed_df.drop(columns=cat_cols, axis=1), \n",
    "     train_dummies], axis=1\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = pd.concat(\n",
    "    [X_test_transformed_df.drop(columns=cat_cols, axis=1),\n",
    "    test_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(X_test_final.columns) != len(X_train_final.columns):\n",
    "    assert ValueError(\"The number of columns of the train and test data frame should match\")\n",
    "else: \n",
    "    print(f'The number of columns in the training set is {len(X_train_final.columns)} and in the testing set is: {len(X_test_final.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train = np.array(X_train_final)\n",
    "X_test = np.array(X_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class rebalancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "from collections import Counter\n",
    "increase_class_1 = 1.4\n",
    "increase_class_2 = 2.5\n",
    "original_counts = Counter(y_train)\n",
    "target_counts = {\n",
    "    1: int(original_counts[1] * increase_class_1),   \n",
    "    2: int(original_counts[2] * increase_class_2)    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_enn = SMOTEENN(sampling_strategy=target_counts, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_enn.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "reduction_proportion = 0.5\n",
    "original_counts = Counter(y_train)\n",
    "target_majority_size = int(original_counts[0] * reduction_proportion)\n",
    "undersampler = RandomUnderSampler(sampling_strategy={0: target_majority_size}, random_state=42)\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"Compute metrics for multiclass classification.\"\"\"\n",
    "    class_report = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "    # Extract precision and recall per class\n",
    "    per_clss_metrics = {\n",
    "        f\"Precision per (Class {cls})\": class_report[cls][\"precision\"]\n",
    "        for cls in class_report if cls not in [\"accuracy\", \"macro avg\", \"weighted avg\"]\n",
    "    }\n",
    "    per_clss_metrics.update({\n",
    "        f\"Recall per (Class {cls})\": class_report[cls][\"recall\"]\n",
    "        for cls in class_report if cls not in [\"accuracy\", \"macro avg\", \"weighted avg\"]\n",
    "    })\n",
    "\n",
    "    # Combine all metrics\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision (Micro)\": precision_score(y_true, y_pred, average='micro'),\n",
    "        \"Precision (Macro)\": precision_score(y_true, y_pred, average='macro'),\n",
    "        \"Recall (Micro)\": recall_score(y_true, y_pred, average='micro'),\n",
    "        \"Recall (Macro)\": recall_score(y_true, y_pred, average='macro'),\n",
    "        \"F1 Score (Micro)\": f1_score(y_true, y_pred, average='micro'),\n",
    "        \"F1 Score (Macro)\": f1_score(y_true, y_pred, average='macro'),\n",
    "        \"Classification Report\": class_report,\n",
    "        \"Confusion Matrix\": confusion_matrix(y_true, y_pred),\n",
    "        **per_clss_metrics  # Add per-class precision and recall\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000, \n",
    "                                             class_weight='balanced'),\n",
    "    \"SVM Balanced\": SVC(probability=True, \n",
    "                        class_weight='balanced'),\n",
    "    \"RandomForest\":RandomForestClassifier(n_estimators=100, \n",
    "                                          class_weight='balanced', \n",
    "                                          random_state=42),\n",
    "    \"Balanced CatBoost\": CatBoostClassifier(auto_class_weights='Balanced', \n",
    "                                            random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom Threshold Classifier strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdClassifier:\n",
    "    def __init__(self, base_classifier, thresholds):\n",
    "        self.base_classifier = base_classifier\n",
    "        self.thresholds = thresholds\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.base_classifier.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probabilities = self.base_classifier.predict_proba(X) \n",
    "        predictions = []\n",
    "        for prob in probabilities:\n",
    "            for i, threshold in enumerate(self.thresholds):\n",
    "                if prob[i] >= threshold:\n",
    "                    predictions.append(self.classes_[i])\n",
    "                    break\n",
    "            else:\n",
    "                predictions.append(self.classes_[np.argmax(prob)])  \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import (OneVsRestClassifier, OneVsOneClassifier, \n",
    "                                OutputCodeClassifier)\n",
    "results = [] \n",
    "cms = []\n",
    "strategies = [\n",
    "    (\"OvR\", OneVsRestClassifier),\n",
    "    (\"OvO\", OneVsOneClassifier),\n",
    "    (\"ECOC\", lambda clf: OutputCodeClassifier(clf, \n",
    "                                              code_size=2, \n",
    "                                              random_state=42)),\n",
    "    (\"CustomThreshold\", lambda clf: ThresholdClassifier(clf, \n",
    "                                                        thresholds=[0.6, 0.5, 0.2])) \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_class(classifiers, strategies, \n",
    "                         X_train, y_train, \n",
    "                         X_test, \n",
    "                         y_test):\n",
    "    results = []\n",
    "    cms = []\n",
    "\n",
    "    for classifier_name, clf in classifiers.items():\n",
    "        for strategy_name, strategy in strategies:\n",
    "            print(f'Applying strategy: {strategy_name} to Classifier: {classifier_name}')\n",
    "            model = strategy(clf)\n",
    "            model.fit(X_train, y_train)\n",
    "            preds = model.predict(X_test)\n",
    "\n",
    "            # Compute metrics\n",
    "            metrics = compute_metrics(y_test, preds)\n",
    "            result = {\n",
    "                'Classifier': classifier_name,\n",
    "                'Strategy': strategy_name,\n",
    "                \"Accuracy\": metrics['Accuracy'],\n",
    "                'Precision (micro)': metrics['Precision (Micro)'],\n",
    "                \"Precision (macro)\": metrics['Precision (Macro)'],\n",
    "                'Recall (micro)': metrics['Recall (Micro)'],\n",
    "                \"Recall (macro)\": metrics['Recall (Macro)'],\n",
    "                \"F1 Score (micro)\": metrics['F1 Score (Micro)'],\n",
    "                \"F1 Score (macro)\": metrics['F1 Score (Macro)']\n",
    "            }\n",
    "\n",
    "            # Add precision per class\n",
    "            for key in metrics:\n",
    "                if key.startswith(\"Precision per (Class\"):\n",
    "                    result[key] = metrics[key]\n",
    "            # Add recall per class\n",
    "            for key in metrics:\n",
    "                if key.startswith(\"Recall per (Class\"):\n",
    "                    result[key] = metrics[key]\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "            cms.append({\n",
    "                \"Classifier\": f'{classifier_name} - {strategy_name}',\n",
    "                \"ConfusionMatrix\": metrics['Confusion Matrix']\n",
    "            })\n",
    "\n",
    "            print(f\"Classification Report for {classifier_name} ({strategy_name}):\\n\")\n",
    "            print(pd.DataFrame(metrics[\"Classification Report\"]).transpose())\n",
    "            print(\"\\n\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df, cms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, cms = train_and_eval_class(classifiers=classifiers, \n",
    "                                       strategies=strategies,\n",
    "                                       X_train=X_train_resampled, \n",
    "                                       y_train=y_train_resampled,\n",
    "                                       y_test=y_test, \n",
    "                                       X_test=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_dataframe(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelviz.confusion_matrix import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = 'RandomForest - OvO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Low risk', 'Medium risk', 'High risk']\n",
    "for idx, cm in enumerate(cms):\n",
    "    if cm['Classifier'] == best_model:\n",
    "        print(f\"Model: {cm['Classifier']} found at index: {idx}\")\n",
    "        plot_confusion_matrix(cm['ConfusionMatrix'], \n",
    "                              model_name=cm['Classifier'], \n",
    "                              classes=classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-book-py-12-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
